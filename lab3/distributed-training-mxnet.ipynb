{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Labs\n",
    "\n",
    "## Lab 3-3 Distributed Training (MXNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelism\n",
    "\n",
    "**data parallelism**\n",
    "to partition the workload over multiple devices. Assume there are n devices. Then each one will receive a copy of the complete model and train it on 1/n of the data. The results such as gradients and updated model are communicated across these devices.\n",
    "\n",
    "**model parallelism**\n",
    "In this approach, each device holds onto only part of the model. \n",
    "\n",
    "> In this lab, data parallelism method is introduced. Refer to https://github.com/dmlc/mxnet/tree/master/example/model-parallel-lstm to see how Model Parallelism could be implemented.\n",
    "\n",
    "<img src='./parallelism-google.png' width='800'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MXNet Distributed Training Architecture\n",
    "\n",
    "* Worker node: \n",
    "* Server node: \n",
    "* Shared storage:\n",
    "\n",
    "> In case of no shared storage available :\n",
    "\n",
    "![](https://raw.githubusercontent.com/dmlc/dmlc.github.io/master/img/ps-arch.png \"ps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MXNet Distributed Training Cluster Types\n",
    "\n",
    "* ssh\n",
    "* mpi\n",
    "* local\n",
    "* sge\n",
    "* yarn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key-Value Store in MXNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is KV Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to set KV Store (http://mxnet.io/api/python/kvstore.html)\n",
    "\n",
    "mxnet.kvstore.create() creates a new KVStore.\n",
    "\n",
    "For **single** machine training, there are two commonly used types:\n",
    "\n",
    "* local: Copies all gradients to CPU memory and updates weights there.\n",
    "\n",
    "* device: Aggregates gradients and updates weights on GPUs. With this setting, the KVStore also attempts to use GPU peer-to-peer communication, potentially accelerating the communication.\n",
    "\n",
    ">Note: <font color='red'>Use \"device\" if GPUs >= 4</font>\n",
    "\n",
    "For **distributed** training, KVStore also supports a number of types:\n",
    "\n",
    "* **dist_sync**: Behaves similarly to local but with one major difference. With dist_sync, batch-size now means the batch size used on each machine. So if there are n machines and we use batch size b, then dist_sync behaves like local with batch size n * b.\n",
    "\n",
    "* **dist_device_sync**: Identical to dist_sync with the difference similar to device vs local.\n",
    "\n",
    "* **dist_async**: Performs asynchronous updates. The weights are updated whenever gradients are received from any machine. No two updates happen on the same weight at the same time. However, the order is not guaranteed.\n",
    "\n",
    "\n",
    "** Souce code: ./common/fit.py **\n",
    "```\n",
    "    # kvstore\n",
    "    kv = mx.kvstore.create(args.kv_store)\n",
    "```\n",
    "\n",
    "```\n",
    "    # create model\n",
    "    model = mx.mod.Module(\n",
    "        context       = devs,\n",
    "        symbol        = network\n",
    "    )\n",
    "    \n",
    "    # run\n",
    "    model.fit(train,\n",
    "        begin_epoch        = args.load_epoch if args.load_epoch else 0,\n",
    "        num_epoch          = args.num_epochs,\n",
    "        eval_data          = val,\n",
    "        eval_metric        = eval_metrics,\n",
    "        kvstore            = kv,\n",
    "        optimizer          = args.optimizer,\n",
    "        optimizer_params   = optimizer_params,\n",
    "        initializer        = initializer,\n",
    "        arg_params         = arg_params,\n",
    "        aux_params         = aux_params,\n",
    "        batch_end_callback = batch_end_callbacks,\n",
    "        epoch_end_callback = checkpoint,\n",
    "        allow_missing      = True,\n",
    "        monitor            = monitor)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MXNet Distributed Training Launcher Tool\n",
    "\n",
    "MXNet provides a launcher tool for distributed training. launch.py is located in ./tools directory.\n",
    "\n",
    "\n",
    "```\n",
    "$ ./launch.py -h\n",
    "usage: launch.py [-h] -n NUM_WORKERS [-s NUM_SERVERS] [-H HOSTFILE]\n",
    "                 [--sync-dst-dir SYNC_DST_DIR]\n",
    "                 [--launcher {local,ssh,mpi,sge,yarn}]\n",
    "                 command [command ...]\n",
    "\n",
    "Launch a distributed job\n",
    "\n",
    "positional arguments:\n",
    "  command               command for launching the program\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -n NUM_WORKERS, --num-workers NUM_WORKERS\n",
    "                        number of worker nodes to be launched\n",
    "  -s NUM_SERVERS, --num-servers NUM_SERVERS\n",
    "                        number of server nodes to be launched, in default it\n",
    "                        is equal to NUM_WORKERS\n",
    "  -H HOSTFILE, --hostfile HOSTFILE\n",
    "                        the hostfile of slave machines which will run the job.\n",
    "                        Required for ssh and mpi launcher\n",
    "  --sync-dst-dir SYNC_DST_DIR\n",
    "                        if specificed, it will sync the current directory into\n",
    "                        slave machines's SYNC_DST_DIR if ssh launcher is used\n",
    "  --launcher {local,ssh,mpi,sge,yarn}\n",
    "                        the launcher to use\n",
    "```\n",
    "\n",
    "Trackers for each launch is located in examples/mxnet/dmlc-core/tracker/dmlc_tracker directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Deep Learning on AWS\n",
    "\n",
    "https://github.com/awslabs/deeplearning-cfn\n",
    "\n",
    "* Master\n",
    "* Worker (/opt/deeplearning/workers)\n",
    "\n",
    "<img src=\"https://github.com/awslabs/deeplearning-cfn/raw/master/images/Slide0.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Steps - MXNet\n",
    "\n",
    "Step 1. Add SSH private key into the authentication agent\n",
    "\n",
    "```\n",
    "$ ssh-add -K <private key file name>\n",
    "```\n",
    "\n",
    "Step 2. SSH to the master node\n",
    "\n",
    "Step 3. Clone git repository\n",
    "\n",
    "Step 4. Execute distributed training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone the awslabs/deeplearning-cfn repo that contains the examples onto the EFS mount\n",
    "\n",
    "```\n",
    "git clone https://github.com/awslabs/deeplearning-cfn $EFS_MOUNT/deeplearning-cfn && \\\n",
    "cd $EFS_MOUNT/deeplearning-cfn && \\\n",
    "#\n",
    "#fetches dmlc/mxnet and tensorflow/models repos as submodules\n",
    "git submodule update --init $EFS_MOUNT/deeplearning-cfn/examples/tensorflow/models && \\\n",
    "git submodule update --init $EFS_MOUNT/deeplearning-cfn/examples/mxnet && \\\n",
    "cd $EFS_MOUNT/deeplearning-cfn/examples/mxnet/ && \\\n",
    "git submodule update --init $EFS_MOUNT/deeplearning-cfn/examples/mxnet/dmlc-core\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Distributed Training on MXNet\n",
    "\n",
    "\n",
    "1. terminate all running Python processes across workers \n",
    "```\n",
    "while read -u 10 host; do ssh -o \"StrictHostKeyChecking no\" $host \"pkill -f python\" ; \\\n",
    "done 10<$DEEPLEARNING_WORKERS_PATH\n",
    "```\n",
    "\n",
    "2. navigate to the MXNet image-classification example directory\n",
    "```\n",
    "cd $EFS_MOUNT/deeplearning-cfn/examples/mxnet/example/image-classification/\n",
    "```\n",
    "\n",
    "3. [GPU] run the CIFAR10 distributed training example\n",
    "```\n",
    "../../tools/launch.py -n $DEEPLEARNING_WORKERS_COUNT -H $DEEPLEARNING_WORKERS_PATH \\\n",
    "python train_cifar10.py --gpus $(seq -s , 0 1 $(($DEEPLEARNING_WORKER_GPU_COUNT - 1))) \\\n",
    "--network resnet --num-layers 50 --kv-store dist_device_sync --num-epoch 2\n",
    "```\n",
    "\n",
    "4. [CPU] run the CIFAR10 distributed training example\n",
    "```\n",
    "../../tools/launch.py -n $DEEPLEARNING_WORKERS_COUNT -H $DEEPLEARNING_WORKERS_PATH \\\n",
    "python train_cifar10.py  \\\n",
    "--network resnet --num-layers 8 --kv-store dist_device_sync --num-epoch 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process started on worker node\n",
    "\n",
    "```\n",
    "ubuntu    1768  1744  1 08:49 ?        00:00:01 python train_cifar10.py --gpus --network resnet --num-layers 50 --kv-store dist_device_sync\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output of Distributed MXNet execution\n",
    "\n",
    "```\n",
    "ubuntu@ip-10-0-0-219:/myEFSvolume/deeplearning-cfn/examples/mxnet/example/image-classification$ ../../tools/launch.py -n $DEEPLEARNING_WORKERS_COUNT -H $DEEPLEARNING_WORKERS_PATH python train_cifar10.py  --network resnet --num-layers 8 --kv-store dist_device_sync --num-epoch 1 --model-prefix /myEFSvolume/deeplearning-cfn/examples/mxnet/example/image-classification/model-output\n",
    "NVIDIA: no NVIDIA devices found\n",
    "NVIDIA: no NVIDIA devices found\n",
    "NVIDIA: no NVIDIA devices found\n",
    "NVIDIA: no NVIDIA devices found\n",
    "NVIDIA: no NVIDIA devices found\n",
    "INFO:root:start with arguments Namespace(batch_size=128, benchmark=0, data_nthreads=4, data_train='data/cifar10_train.rec', data_val='data/cifar10_val.rec', disp_batches=20, dtype='float32', gpus=None, image_shape='3,28,28', kv_store='dist_device_sync', load_epoch=None, lr=0.05, lr_factor=0.1, lr_step_epochs='200,250', max_random_aspect_ratio=0, max_random_h=36, max_random_l=50, max_random_rotate_angle=0, max_random_s=50, max_random_scale=1, max_random_shear_ratio=0, min_random_scale=1, model_prefix='/myEFSvolume/deeplearning-cfn/examples/mxnet/example/image-classification/model-output', mom=0.9, monitor=0, network='resnet', num_classes=10, num_epochs=1, num_examples=50000, num_layers=8, optimizer='sgd', pad_size=4, random_crop=1, random_mirror=1, rgb_mean='123.68,116.779,103.939', test_io=0, top_k=0, wd=0.0001)\n",
    "INFO:root:start with arguments Namespace(batch_size=128, benchmark=0, data_nthreads=4, data_train='data/cifar10_train.rec', data_val='data/cifar10_val.rec', disp_batches=20, dtype='float32', gpus=None, image_shape='3,28,28', kv_store='dist_device_sync', load_epoch=None, lr=0.05, lr_factor=0.1, lr_step_epochs='200,250', max_random_aspect_ratio=0, max_random_h=36, max_random_l=50, max_random_rotate_angle=0, max_random_s=50, max_random_scale=1, max_random_shear_ratio=0, min_random_scale=1, model_prefix='/myEFSvolume/deeplearning-cfn/examples/mxnet/example/image-classification/model-output', mom=0.9, monitor=0, network='resnet', num_classes=10, num_epochs=1, num_examples=50000, num_layers=8, optimizer='sgd', pad_size=4, random_crop=1, random_mirror=1, rgb_mean='123.68,116.779,103.939', test_io=0, top_k=0, wd=0.0001)\n",
    "[16:27:44] src/io/iter_image_recordio_2.cc:135: ImageRecordIOParser2: data/cifar10_train.rec, use 1 threads for decoding..\n",
    "[16:27:44] src/io/iter_image_recordio_2.cc:135: ImageRecordIOParser2: data/cifar10_train.rec, use 1 threads for decoding..\n",
    "[16:27:44] src/io/iter_image_recordio_2.cc:135: ImageRecordIOParser2: data/cifar10_val.rec, use 1 threads for decoding..\n",
    "[16:27:44] src/io/iter_image_recordio_2.cc:135: ImageRecordIOParser2: data/cifar10_val.rec, use 1 threads for decoding..\n",
    "INFO:root:Epoch[0] Batch [20]\tSpeed: 223.74 samples/sec\taccuracy=0.168155\n",
    "INFO:root:Epoch[0] Batch [20]\tSpeed: 224.03 samples/sec\taccuracy=0.165551\n",
    "INFO:root:Epoch[0] Batch [40]\tSpeed: 219.29 samples/sec\taccuracy=0.258984\n",
    "INFO:root:Epoch[0] Batch [40]\tSpeed: 218.81 samples/sec\taccuracy=0.233203\n",
    "INFO:root:Epoch[0] Batch [60]\tSpeed: 219.94 samples/sec\taccuracy=0.276953\n",
    "INFO:root:Epoch[0] Batch [60]\tSpeed: 220.39 samples/sec\taccuracy=0.272656\n",
    "INFO:root:Epoch[0] Batch [80]\tSpeed: 214.74 samples/sec\taccuracy=0.298047\n",
    "INFO:root:Epoch[0] Batch [80]\tSpeed: 214.70 samples/sec\taccuracy=0.300391\n",
    "INFO:root:Epoch[0] Batch [100]\tSpeed: 223.44 samples/sec\taccuracy=0.319141\n",
    "INFO:root:Epoch[0] Batch [100]\tSpeed: 223.35 samples/sec\taccuracy=0.319531\n",
    "INFO:root:Epoch[0] Batch [120]\tSpeed: 213.90 samples/sec\taccuracy=0.331641\n",
    "INFO:root:Epoch[0] Batch [120]\tSpeed: 213.67 samples/sec\taccuracy=0.320703\n",
    "INFO:root:Epoch[0] Batch [140]\tSpeed: 228.06 samples/sec\taccuracy=0.348438\n",
    "INFO:root:Epoch[0] Batch [140]\tSpeed: 225.88 samples/sec\taccuracy=0.338281\n",
    "INFO:root:Epoch[0] Batch [160]\tSpeed: 217.08 samples/sec\taccuracy=0.368359\n",
    "INFO:root:Epoch[0] Batch [160]\tSpeed: 215.39 samples/sec\taccuracy=0.361719\n",
    "INFO:root:Epoch[0] Batch [180]\tSpeed: 204.91 samples/sec\taccuracy=0.364844\n",
    "INFO:root:Epoch[0] Batch [180]\tSpeed: 203.97 samples/sec\taccuracy=0.387500\n",
    "INFO:root:Epoch[0] Train-accuracy=0.382812\n",
    "INFO:root:Epoch[0] Time cost=114.941\n",
    "INFO:root:Epoch[0] Train-accuracy=0.380208\n",
    "INFO:root:Epoch[0] Time cost=114.961\n",
    "INFO:root:Saved checkpoint to \"/myEFSvolume/deeplearning-cfn/examples/mxnet/example/image-classification/model-output-0001.params\"\n",
    "INFO:root:Saved checkpoint to \"/myEFSvolume/deeplearning-cfn/examples/mxnet/example/image-classification/model-output-1-0001.params\"\n",
    "INFO:root:Epoch[0] Validation-accuracy=0.425977\n",
    "INFO:root:Epoch[0] Validation-accuracy=0.427734\n",
    "MKL Build:20170209\n",
    "MKL Build:20170209\n",
    "```\n",
    "\n",
    "> ### Trainig output using a single machine\n",
    ">\n",
    "With the same parameter (num-layers, num-epoch), the training on a single machine \n",
    "```\n",
    "INFO:root:Epoch[0] Batch [20]\tSpeed: 363.66 samples/sec\taccuracy=0.164807\n",
    "INFO:root:Epoch[0] Batch [40]\tSpeed: 353.78 samples/sec\taccuracy=0.228516\n",
    "INFO:root:Epoch[0] Batch [60]\tSpeed: 363.93 samples/sec\taccuracy=0.264453\n",
    "INFO:root:Epoch[0] Batch [80]\tSpeed: 354.36 samples/sec\taccuracy=0.278906\n",
    "INFO:root:Epoch[0] Batch [100]\tSpeed: 364.05 samples/sec\taccuracy=0.299219\n",
    "INFO:root:Epoch[0] Batch [120]\tSpeed: 352.75 samples/sec\taccuracy=0.308594\n",
    "INFO:root:Epoch[0] Batch [140]\tSpeed: 364.74 samples/sec\taccuracy=0.343750\n",
    "INFO:root:Epoch[0] Batch [160]\tSpeed: 356.02 samples/sec\taccuracy=0.339844\n",
    "INFO:root:Epoch[0] Batch [180]\tSpeed: 365.27 samples/sec\taccuracy=0.349219\n",
    "INFO:root:Epoch[0] Batch [200]\tSpeed: 356.39 samples/sec\taccuracy=0.348047\n",
    "INFO:root:Epoch[0] Batch [220]\tSpeed: 364.78 samples/sec\taccuracy=0.364844\n",
    "INFO:root:Epoch[0] Batch [240]\tSpeed: 354.15 samples/sec\taccuracy=0.371094\n",
    "INFO:root:Epoch[0] Batch [260]\tSpeed: 364.89 samples/sec\taccuracy=0.364063\n",
    "INFO:root:Epoch[0] Batch [280]\tSpeed: 356.20 samples/sec\taccuracy=0.388672\n",
    "INFO:root:Epoch[0] Batch [300]\tSpeed: 364.59 samples/sec\taccuracy=0.404687\n",
    "INFO:root:Epoch[0] Batch [320]\tSpeed: 353.17 samples/sec\taccuracy=0.414453\n",
    "INFO:root:Epoch[0] Batch [340]\tSpeed: 362.48 samples/sec\taccuracy=0.423828\n",
    "INFO:root:Epoch[0] Batch [360]\tSpeed: 360.31 samples/sec\taccuracy=0.415234\n",
    "INFO:root:Epoch[0] Batch [380]\tSpeed: 353.27 samples/sec\taccuracy=0.435937\n",
    "INFO:root:Epoch[0] Train-accuracy=0.407813\n",
    "INFO:root:Epoch[0] Time cost=138.994\n",
    "INFO:root:Epoch[0] Validation-accuracy=0.471519\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model output\n",
    "\n",
    "* model-output-symbol.json\n",
    "* model-output-1-symbol.json\n",
    "* model-output-0001.params\n",
    "* model-output-1-0001.params\n",
    "\n",
    "> <font color='red'>Why does models for each epoch of node give different validation accuracy ?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the training speed and accuracy of distributed training vs. single node training\n",
    "\n",
    "Number of workers | Time | Speed (samples/sec) of each batch (approx) | Validation Accuracy\n",
    ":---:| :---: | :---: | :--:\n",
    "1 (Single CPU machine, c4.large) | 138 | 350 | 0.47\n",
    "2 (2 node CPU cluster, c4.large)| 114 | 210 | 0.42\n",
    "2 (4 node CPU cluster, c4.large) | 73 | 160 | 0.52\n",
    "2 (2 node GPU cluster, p2.xlarge) | 29 | 880 | 0.53\n",
    "4 (4 node CPU cluster, c4.large) | 36 | 340 | 0.44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Run MXNet on Multiple CPU/GPUs with Data Parallelism\n",
    "http://mxnet.io/how_to/multi_devices.html\n",
    "\n",
    "Distributed Training\n",
    "http://newdocs.readthedocs.io/en/latest/distributed_training.html\n",
    "\n",
    "Distributed TensorFlow \n",
    "https://www.tensorflow.org/deploy/distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multiple GPUs in a Single Machine\n",
    "\n",
    "##### Key-Value Store Types\n",
    "* **local** : all gradients are copied to CPU memory and weights are updated there\n",
    "* **device** : both gradient aggregation and weight updates are run on GPUs. With this setting, the KVStore also attempts to use GPU peer-to-peer communication, potentially accelerating the communication. Note that this option may result in higher GPU memory usage. \n",
    "\n",
    ">Note: <font color='red'>Use \"device\" if GPUs >= 4</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
