{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 Image Classification Labs\n",
    "\n",
    "## Lab 3-1 Training ResNet with CIFAR 10 dataset using MXNet (Single Node)\n",
    "\n",
    "This lab is to train ResNet neural network with CIFAR-10 training data to classify an image into 10 known categories. The code is written in MXNet, and it has been modified to use S3 bucket instead of a local file system.\n",
    "\n",
    "> **AWS Deep Learning AMI Ubuntu Version (1.5_Jun2017)**\n",
    ">\n",
    "> * Upgraded to latest Ubuntu base AMI for 14.04\n",
    "> * MXNet compiled with **S3 Support**\n",
    "> * 7 Deep Learning Frameworks - contains the most popular Deep Learning Frameworks (MXNet, Caffe, Caffe2, Tensorflow, Theano, Torch and CNTK)\n",
    "\n",
    "\n",
    "This python source code is based on *example/image-classification/train_cifar10.py* which is included in mxnet package of Deep Learning AMI. Below is the code changes made for S3 support.\n",
    "\n",
    "* In train_cifar10.py, *download_cifar10()* function is modified to use the training data from Amazon S3 bucket.\n",
    "\n",
    "```python\n",
    "def download_cifar10():\n",
    "    #data_dir=\"data\"\n",
    "    data_dir=\"s3://<S3 bucket name>/deeplearning\"\n",
    "    fnames = (os.path.join(data_dir, \"cifar10_train.rec\"),\n",
    "              os.path.join(data_dir, \"cifar10_val.rec\"))\n",
    "    #download_file('http://data.mxnet.io/data/cifar10/cifar10_val.rec', fnames[1])\n",
    "    #download_file('http://data.mxnet.io/data/cifar10/cifar10_train.rec', fnames[0])\n",
    "    return fnames\n",
    "```\n",
    "\n",
    "* In common/fit.py, 3 lines are commented out which are not for S3.\n",
    "\n",
    "```python\n",
    "def _save_model(args, rank=0):\n",
    "    if args.model_prefix is None:\n",
    "        return None\n",
    "    #dst_dir = os.path.dirname(args.model_prefix)\n",
    "    #if not os.path.isdir(dst_dir):\n",
    "    #    os.mkdir(dst_dir)\n",
    "    return mx.callback.do_checkpoint(args.model_prefix if rank == 0 else \"%s-%d\" % (\n",
    "        args.model_prefix, rank))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do with images \n",
    "\n",
    "* Classification\n",
    "* Localization\n",
    "* Segmentation\n",
    "* Scene classification\n",
    "* [Scene parsing](http://sceneparsing.csail.mit.edu/) : to segment and parse an image into different image regions associated with semantic categories, such as sky, road, person, and bed\n",
    "\n",
    "<img src='./computer-vision-tasks.png' width=600>\n",
    "\n",
    "If you want to learn more about deep learning on image, here is a good lecture [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Look at Image Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 (https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "\n",
    "CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. \n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
    "\n",
    "Here are the classes in the dataset, as well as 10 random images from each:\n",
    "\n",
    "<img src=\"cifar10-classes.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageNet (http://www.image-net.org/) and ImageNet Large Scale Visual Recognition Challenge (ILSVRC)\n",
    "\n",
    "An image database organized according to the [WordNet, a large lexical database of English,](https://wordnet.princeton.edu/) hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images. Currently we have an average of over five hundred images per node.\n",
    "\n",
    "* 14,197,122 images, 21841 synsets indexed (as of July 5)\n",
    "* For example, you can search ImagetNet for a synset (or synonym set), car - http://www.image-net.org/search?q=car\n",
    "\n",
    "#### ILSVRC History Winner\n",
    "\n",
    "* 2012 : AlexNet (https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "* 2013 : ZFNet (https://arxiv.org/pdf/1311.2901v3.pdf)\n",
    "* 2014 : GoogLeNet (http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)\n",
    "* 2015 : Residual Neural Network (ResNet) (https://arxiv.org/pdf/1512.03385v1.pdf)\n",
    "* 2016 : CUImage\n",
    "\n",
    "> Want to learn more about DL? [The 9 Deep Learning Papers You Need To Know About](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)\n",
    "\n",
    "#### ILSVRC 2016\n",
    " * Object localization for 1000 categories.\n",
    " * Object detection for 200 fully labeled categories.\n",
    " * Object detection from video for 30 fully labeled categories.\n",
    " * Scene classification for 365 scene categories \n",
    " * Scene parsing for 150 stuff and discrete object categories "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapillary Vistas Dataset and Large-Scale Scene Understanding (LSUN) Challenge\n",
    "\n",
    "> [**AWS AI Blog**](https://aws.amazon.com/blogs/ai/) AWS Partners with Mapillary to Support the Large-Scale Scene Understanding Challenge at CVPR 2017\n",
    ">\n",
    "> https://aws.amazon.com/blogs/ai/aws-partners-with-mapillary-to-support-the-large-scale-scene-understanding-challenge-at-cvpr-2017/#more-1096\n",
    "\n",
    "#### Mapillary (https://www.mapillary.com/dataset/vistas?lat=20&lng=0&z=1.5)\n",
    "\n",
    "* the worldâ€™s largest and most diverse publicly available, pixel-accurately and instance-specifically annotated <font color='blue'>street-level imagery dataset</font> for empowering autonomous mobility and transport at the global scale\n",
    "* 25,000 Images | 100 Categories | 60 Instance-wise Categories | 6 Continents | Variety of Weather, Season, Time of Day, Camera, and Viewpoint\n",
    "\n",
    "<img src='https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2017/06/28/mapillary.gif' width=400>\n",
    "\n",
    "#### LSUN Challenge (http://lsun.cs.princeton.edu/2017/)\n",
    "\n",
    "* focusing on major tasks in scene understanding\n",
    " * scene classification\n",
    " * scene segmentation\n",
    " * saliency prediction\n",
    " * RGB-D detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO, Common Objects in Content (http://mscoco.org/)\n",
    "\n",
    "a new image recognition, segmentation, and captioning dataset. \n",
    "\n",
    "COCO 2016 Detection and Keypoint Challenges\n",
    "\n",
    "<img src='http://mscoco.org/static/images/dataset.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet (Residual Net) [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "* residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity\n",
    "* achieves 3.57% error on the ImageNet test set\n",
    "* 1st place on the ILSVRC (Large Scale Visual Recognition Challenge) 2015 classification task\n",
    "\n",
    "<img src=\"new-resnet-arch.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Steps of  Lab 3-1\n",
    "\n",
    "<img src='./lab3-1-figure-1.png' width=400>\n",
    "\n",
    "Here I assume you've cloned CodeCommit repository to your EC2 instances (Deep Learning AMI)\n",
    "\n",
    "**Step 1** Create an Amazon S3 bucket for training data and checkpoint\n",
    "\n",
    "**Step 2** Upload data/cifar10_train.rec and data/ficar10_val.rec into the S3 bucket (Use S3 prefix as *deeplearning/*)\n",
    "\n",
    "```\n",
    "$ cd lab3/mxnet-resnet_cifar10/data\n",
    "$ aws s3 cp cifar10_train.rec s3://<bucket-name>/deeplearning/cifar10_train.rec\n",
    "$ aws s3 cp cifar10_val.rec s3://<bucket-name>/deeplearning/cifar10_val.rec\n",
    "```\n",
    "**Step 3** Config AWS credential for Amazon S3 access \n",
    "\n",
    "> **NOTE** MXNet does not support EC2 role. AWS Credential along with the region should be defined as environment variables.\n",
    "\n",
    "```\n",
    "$ export AWS_ACCESS_KEY_ID=\n",
    "$ export AWS_SECRET_ACCESS_KEY=\n",
    "$ export AWS_REGION=\n",
    "```\n",
    "\n",
    "**Step 4** Set S3 bucket name within train_cifar10.py.\n",
    "\n",
    "```python\n",
    "  9 def download_cifar10():\n",
    " 10     #data_dir=\"data\"\n",
    " 11     data_dir=\"s3://<YOUR S3 BUCKET NAME HERE>/deeplearning\"\n",
    " 12     fnames = (os.path.join(data_dir, \"cifar10_train.rec\"),\n",
    " 13               os.path.join(data_dir, \"cifar10_val.rec\"))\n",
    " 14     #download_file('http://data.mxnet.io/data/cifar10/cifar10_val.rec', fnames[1])\n",
    " 15     #download_file('http://data.mxnet.io/data/cifar10/cifar10_train.rec', fnames[0])\n",
    " 16     return fnames\n",
    "```\n",
    "\n",
    "**Step 5** Run train_cifar10.py using the below command\n",
    "\n",
    "This will load CIFAR-10 load training and validation data set from S3 bucket (Step 2), and train ResNet with 8 hidden layers. Once the training is completed, the model checkpoint will be saved to the specified S3 bucket (--model-prefix).\n",
    "\n",
    "```\n",
    "$ python ./train_cifar10.py \\\n",
    "    --model-prefix s3://<bucket-name>/deeplearning/trained-model/mxnet-resnet-cifar10 \\\n",
    "    --num-layers 8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional Task] Upload trained model into S3 (if S3 is not supported by MXNet)\n",
    "\n",
    "For the Lab 4, upload trained model files (symbol and parameter) into S3 bucket. If you specify S3 bucket name in --model-prefix parameter, you do not need to upload the files. Just check the files in the S3 bucket.\n",
    "\n",
    "```\n",
    "$ aws s3 sync ./trained-model s3://<bucket name>/deeplearning\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional Task] Change parameters and observe how the training is changing\n",
    "\n",
    "* num_layers : how many hidden layers to have in the ResNet\n",
    "* lr : learning rate\n",
    "* batch_size : the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional Task] Use different EC2 instance size (c4.large, c4.xlarge, c4.2xlarge, c4.4xlarge, and c4.8xlarge)\n",
    "\n",
    "\n",
    "Instace size | Speed (samples/sec for each batch) | Time cost | Train accuracy | Valiation accuracy \n",
    ":---:| :---: | :---: | :---: | :---:\n",
    "c4.large | 000 | 000 | 0.00 | 0.00\n",
    "c4.xlarge | 000 | 000 | 0.00 | 0.00\n",
    "c4.2xlarge | 000 | 000 | 0.00 | 0.00\n",
    "c4.4xlarge | 000 | 000 | 0.00 | 0.00\n",
    "c4.8xlarge | 000 | 000 | 0.00 | 0.00\n",
    "p2.xlarge | 000 | 000 | 0.00 | 0.00\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing necessary modules including mxnet\n",
    "\n",
    "find_mxnet, data, and fit module are located in common directory, and download_file is in common/util.py. If you are curious about how to implement ResNet, take a look at common/fit.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "from common import find_mxnet, data, fit\n",
    "from common.util import download_file\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the location of training dataset\n",
    "\n",
    "The original code downloads the training dataset from mxnet.io and store them into the local file system. In this example, it is modified to use training data in Amazon S3 bucket.\n",
    "\n",
    "Refer to *Use data from S3 for training* (http://mxnet.io/how_to/s3_integration.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_cifar10():\n",
    "    data_dir=\"s3://<bucket-name>/deeplearning\"\n",
    "    fnames = (os.path.join(data_dir, \"cifar10_train.rec\"),\n",
    "              os.path.join(data_dir, \"cifar10_val.rec\"))\n",
    "    return fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare the training dataset, parse arguments for training\n",
    "\n",
    "This ResNet implementation could be tuned through parameters such as;\n",
    "\n",
    "* network : the name of neural network to use in the training. Default is ResNet.\n",
    "* num_layers : the number of hidden layers. Keep it small for this lab. More layers, higher accuracy, but taking long time for training\n",
    "* num_classes : the number of classificiation targets (CIFAR-10 has 10 classes while CIFAR-100 has 100)\n",
    "* image_shape : input image shape in (number of channels, pixels in height, pixels in width). For CIFAR-10, use (3,28,28)\n",
    "* batch_size : the size of mini-batch\n",
    "\n",
    "> **NOTE** Stochastic Gradient Descent, Stochastic Gradient Descent\n",
    ">\n",
    "> * Batch Gradient Descent : Use all n examples in each iteration for a parameter update\n",
    "> * Stochastic Gradient Descent : Use 1 example in each iteration for a parameter update\n",
    "> * Mini-Batch Gradient Descent : Use b example in each iteration for a parameter update\n",
    ">\n",
    "> https://visualstudiomagazine.com/articles/2014/08/01/batch-training.aspx\n",
    ">\n",
    "> https://www.quora.com/What-are-the-meanings-of-batch-size-mini-batch-iterations-and-epoch-in-neural-networks\n",
    "* num_epochs :\n",
    "* lr : learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download data\n",
    "(train_fname, val_fname) = download_cifar10()\n",
    "   \n",
    "# parse args\n",
    "parser = argparse.ArgumentParser(description=\"train cifar10\",\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter,conflict_handler='resolve')\n",
    "fit.add_fit_args(parser)\n",
    "data.add_data_args(parser)\n",
    "data.add_data_aug_args(parser)\n",
    "data.set_data_aug_level(parser, 2)\n",
    "\n",
    "parser.set_defaults(\n",
    "    # network\n",
    "    network        = 'resnet',\n",
    "    num_layers     = 110,\n",
    "    #num_layers     = 5,\n",
    "    # data\n",
    "    data_train     = train_fname,\n",
    "    data_val       = val_fname,\n",
    "    num_classes    = 10,\n",
    "    num_examples  = 50000,\n",
    "    image_shape    = '3,28,28',\n",
    "    pad_size       = 4,\n",
    "    # train\n",
    "    batch_size     = 128,\n",
    "    num_epochs     = 300,\n",
    "    lr             = .05,\n",
    "    lr_step_epochs = '200,250',\n",
    ")\n",
    "\n",
    "args = parser.parse_known_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the pre-defined ResNet module\n",
    "\n",
    "ResNet is defined in a python code, symbols/resnet.py in this example. There are other nerual network definitions in symbols directory; *AlexNet, GoogleNet, Inception, LeNet, VGG, and MLP.*\n",
    "\n",
    "Trained parameters are saved at the end of each epoch, so that a prediction can be made using the trained model. We will do this part in Lab 4 using AWS Lambda. The original example saves checkpoints into the local directory, but this workshop code modified common/fit.py to save the checkpoint into Amazon S3 bucket.\n",
    "\n",
    "The checkpoint files are;\n",
    "\n",
    "* **Symbol file (network design and graph)**: prefix_symbols.json\n",
    "* **Parameters (or weights of NN)** : prefix_{epoch count}.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load network\n",
    "from importlib import import_module\n",
    "net = import_module('symbols.'+args.network)\n",
    "sym = net.get_symbol(**vars(args))\n",
    "\n",
    "model_prefix = 'mx_mlp'\n",
    "checkpoint = mx.callback.do_checkpoint(model_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run training\n",
    "\n",
    "Training data is split into batches, and it repeats training the whole dataset num_epochs times. Each iteration is called as epoch. Upon every epoch completion, it runs a validation againt the validation dataset and gives out the accuracy.\n",
    "\n",
    "> **NOTE** There are two accuracy values are give for each epoch; Traning accuracy and Validation accuracy. \n",
    ">\n",
    "> * What do you expect from these two? \n",
    "> * What if training accuray gets higher while validation accuracy does not? \n",
    "> * What if training accuracy is saturated? \n",
    ">\n",
    "> In that case, what you are going to do?\n",
    "\n",
    "```\n",
    "$ python ./train_cifar10.py --model-prefix s3://<bucket-name>/deeplearning/trained-model/mxnet-resnet-cifar10 --num-layers 8\n",
    "<Symbol softmax>\n",
    "INFO:root:start with arguments Namespace(batch_size=128, benchmark=0, data_nthreads=4, data_train='data/cifar10_train.rec', data_val='data/cifar10_val.rec', disp_batches=20, dtype='float32', gpus=None, image_shape='3,28,28', kv_store='device', load_epoch=None, lr=0.05, lr_factor=0.1, lr_step_epochs='200,250', max_random_aspect_ratio=0, max_random_h=36, max_random_l=50, max_random_rotate_angle=0, max_random_s=50, max_random_scale=1, max_random_shear_ratio=0, min_random_scale=1, model_prefix='./trained-model/mxnet-resnet-cifar10', mom=0.9, monitor=0, network='resnet', num_classes=10, num_epochs=300, num_examples=50000, num_layers=8, optimizer='sgd', pad_size=4, random_crop=1, random_mirror=1, rgb_mean='123.68,116.779,103.939', test_io=0, top_k=0, wd=0.0001)\n",
    "[14:37:14] src/io/iter_image_recordio_2.cc:135: ImageRecordIOParser2: data/cifar10_train.rec, use 1 threads for decoding..\n",
    "[14:37:15] src/io/iter_image_recordio_2.cc:135: ImageRecordIOParser2: data/cifar10_val.rec, use 1 threads for decoding..\n",
    "INFO:root:Epoch[0] Batch [20]\tSpeed: 148.28 samples/sec\taccuracy=0.171875\n",
    "INFO:root:Epoch[0] Batch [40]\tSpeed: 180.31 samples/sec\taccuracy=0.225781\n",
    "INFO:root:Epoch[0] Batch [60]\tSpeed: 208.81 samples/sec\taccuracy=0.255469\n",
    "INFO:root:Epoch[0] Batch [80]\tSpeed: 120.90 samples/sec\taccuracy=0.267969\n",
    "INFO:root:Epoch[0] Batch [100]\tSpeed: 168.35 samples/sec\taccuracy=0.302734\n",
    "INFO:root:Epoch[0] Batch [120]\tSpeed: 169.01 samples/sec\taccuracy=0.311719\n",
    "INFO:root:Epoch[0] Batch [140]\tSpeed: 229.92 samples/sec\taccuracy=0.309375\n",
    "INFO:root:Epoch[0] Batch [160]\tSpeed: 253.50 samples/sec\taccuracy=0.330078\n",
    "INFO:root:Epoch[0] Batch [180]\tSpeed: 182.88 samples/sec\taccuracy=0.331641\n",
    "INFO:root:Epoch[0] Batch [200]\tSpeed: 186.12 samples/sec\taccuracy=0.348828\n",
    "INFO:root:Epoch[0] Batch [220]\tSpeed: 134.41 samples/sec\taccuracy=0.368750\n",
    "INFO:root:Epoch[0] Batch [240]\tSpeed: 165.69 samples/sec\taccuracy=0.365625\n",
    "INFO:root:Epoch[0] Batch [260]\tSpeed: 209.33 samples/sec\taccuracy=0.387500\n",
    "INFO:root:Epoch[0] Batch [280]\tSpeed: 188.32 samples/sec\taccuracy=0.381250\n",
    "INFO:root:Epoch[0] Batch [300]\tSpeed: 192.40 samples/sec\taccuracy=0.420703\n",
    "INFO:root:Epoch[0] Batch [320]\tSpeed: 159.61 samples/sec\taccuracy=0.417578\n",
    "INFO:root:Epoch[0] Batch [340]\tSpeed: 127.66 samples/sec\taccuracy=0.431250\n",
    "INFO:root:Epoch[0] Batch [360]\tSpeed: 178.55 samples/sec\taccuracy=0.446484\n",
    "INFO:root:Epoch[0] Batch [380]\tSpeed: 205.05 samples/sec\taccuracy=0.443359\n",
    "INFO:root:Epoch[0] Train-accuracy=0.433594\n",
    "INFO:root:Epoch[0] Time cost=288.064\n",
    "INFO:root:Saved checkpoint to \"./trained-model/mxnet-resnet-cifar10-0001.params\"\n",
    "INFO:root:Epoch[0] Validation-accuracy=0.493275\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "fit.fit(args, sym, data.get_rec_iter, epoch_end_callback=checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
