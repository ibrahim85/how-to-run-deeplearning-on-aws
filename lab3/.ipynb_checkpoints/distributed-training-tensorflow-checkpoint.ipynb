{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Labs\n",
    "\n",
    "## Lab 3-4 Distributed Training (TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Steps - TensorFlow\n",
    "\n",
    "Step 1. Add SSH private key into the authentication agent (Skip this step if it is done at Dist MXNet lab)\n",
    "\n",
    "```\n",
    "$ ssh-add -K <private key file name>\n",
    "```\n",
    "\n",
    "Step 2. SSH to the master node\n",
    "\n",
    "Step 3. Clone git repository\n",
    "\n",
    "Step 4. Create command files for each workers\n",
    "\n",
    "Step 5. Run command files on all workers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy training data into EFS directory\n",
    "\n",
    "```\n",
    "mkdir $EFS_MOUNT/cifar10_data && \\\n",
    "wget http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz --directory-prefix=$EFS_MOUNT/cifar10_data \\\n",
    "&& tar -xzvf $EFS_MOUNT/cifar10_data/cifar-10-binary.tar.gz -C $EFS_MOUNT/cifar10_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate commands to be run TensorFlow workers and parameter servers on cluster instances\n",
    "\n",
    "```\n",
    "cd $EFS_MOUNT/deeplearning-cfn/examples/tensorflow && \\\n",
    "# generates commands to run workers and parameter-servers on all the workers \\\n",
    "python generate_trainer.py --workers_file_path $DEEPLEARNING_WORKERS_PATH \\\n",
    "--worker_count $DEEPLEARNING_WORKERS_COUNT \\\n",
    "--worker_gpu_count $DEEPLEARNING_WORKER_GPU_COUNT \\\n",
    "--trainer_script_dir $EFS_MOUNT/deeplearning-cfn/examples/tensorflow \\\n",
    "--training_script $EFS_MOUNT/deeplearning-cfn/examples/tensorflow/cifar10_multi_machine_train.py \\\n",
    "--batch_size 128 --data_dir=$EFS_MOUNT/cifar10_data \\\n",
    "--train_dir=$EFS_MOUNT/deeplearning-cfn/examples/tensorflow/train \\\n",
    "--log_dir $EFS_MOUNT/deeplearning-cfn/examples/tensorflow/logs \\\n",
    "--max_steps 200000\n",
    "```\n",
    "\n",
    "The resulting command file, deeplearning-worker#.sh, is created for each worker.\n",
    "\n",
    "```\n",
    "deeplearning-worker1.sh\n",
    "source /etc/profile\n",
    "\n",
    "CUDA_VISIBLE_DEVICES='' python /myEFSvolume/deeplearning-cfn/examples/tensorflow/cifar10_multi_machine_train.py --batch_size 128 --data_dir=/myEFSvolume/cifar10_data --train_dir=/myEFSvolume/deeplearning-cfn/examples/tensorflow/train --ma\n",
    "x_steps 200000 --ps_hosts=deeplearning-worker1:2222,deeplearning-worker2:2222,deeplearning-worker3:2222,deeplearning-worker4:2222 --worker_hosts= --job_name=ps --task_index=0 > /myEFSvolume/deeplearning-cfn/examples/tensorflow/logs/ps0 2>&1 &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training command file on all workers\n",
    "\n",
    "1. terminate all running Python processes across workers \\\n",
    "```\n",
    "while read -u 10 host; do ssh -o \"StrictHostKeyChecking no\" $host \"pkill -f python\" ; \\\n",
    "done 10<$DEEPLEARNING_WORKERS_PATH\n",
    "```\n",
    "\n",
    "2. Run the distributed training across all of the workers:\n",
    "```\n",
    "trainer_script_dir=$EFS_MOUNT/deeplearning-cfn/examples/tensorflow && while read -u 10 host; \\\n",
    "do ssh -o \"StrictHostKeyChecking no\" $host \"bash $trainer_script_dir/$host.sh\" ; \\\n",
    "done 10<$DEEPLEARNING_WORKERS_PATH\n",
    "```\n",
    "\n",
    ">ubuntu    2047     1  3 19:03 ?        00:00:01 python /myEFSvolume/deeplearning-cfn/examples/tensorflow/cifar10_multi_machine_train.py --batch_size 128 --data_dir=/myEFSvolume/cifar10_data --train_dir=/myEFSvolume/deeplearning-cfn/examples/tensorflow/train --max_steps 200000 --ps_hosts=deeplearning-worker1:2222,deeplearning-worker2:2222 --worker_hosts=deeplearning-worker1:2230,deeplearning-worker2:2230 --job_name=ps --task_index=0\n",
    ">\n",
    ">ubuntu    2048     1  4 19:03 ?        00:00:01 python /myEFSvolume/deeplearning-cfn/examples/tensorflow/cifar10_multi_machine_train.py --batch_size 128 --data_dir=/myEFSvolume/cifar10_data --train_dir=/myEFSvolume/deeplearning-cfn/examples/tensorflow/train --max_steps 200000 --ps_hosts=deeplearning-worker1:2222,deeplearning-worker2:2222 --worker_hosts=deeplearning-worker1:2230,deeplearning-worker2:2230 --job_name=worker --task_index=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### How to monitor GPU usage\n",
    "\n",
    "```\n",
    "$ nvidia-smi\n",
    "Mon Jul  3 19:14:06 2017\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla K80           On   | 0000:00:1E.0     Off |                    0 |\n",
    "| N/A   43C    P0    77W / 149W |  10935MiB / 11439MiB |     58%      Default |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                       GPU Memory |\n",
    "|  GPU       PID  Type  Process name                               Usage      |\n",
    "|=============================================================================|\n",
    "|    0      2048    C   python                                       10931MiB |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```\n",
    "\n",
    "watch -n 0.5 nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating trained model\n",
    "\n",
    "1. terminate all running Python processes across workers \\\n",
    "```\n",
    "while read -u 10 host; do ssh -o \"StrictHostKeyChecking no\" $host \"pkill -f python\" ; \\\n",
    "done 10<$DEEPLEARNING_WORKERS_PATH\n",
    "```\n",
    "\n",
    "2. Run the evaluation on the trained mode\n",
    "```\n",
    "python $EFS_MOUNT/deeplearning-cfn/examples/tensorflow/models/tutorials/image/cifar10/cifar10_eval.py \\\n",
    "--data_dir=$EFS_MOUNT/cifar10_data/ \\\n",
    "--eval_dir=$EFS_MOUNT/deeplearning-cfn/examples/tensorflow/eval \\\n",
    "--checkpoint_dir=$EFS_MOUNT/deeplearning-cfn/examples/tensorflow/train\n",
    "```\n",
    "\n",
    "Sameple output\n",
    "```\n",
    "2017-07-03 19:15:44.131450: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-07-03 19:15:44.131492: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-07-03 19:15:44.131501: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-07-03 19:15:44.131507: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-07-03 19:15:44.131514: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
    "2017-07-03 19:15:44.278553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
    "2017-07-03 19:15:44.279070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties:\n",
    "name: Tesla K80\n",
    "major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n",
    "pciBusID 0000:00:1e.0\n",
    "Total memory: 11.17GiB\n",
    "Free memory: 11.11GiB\n",
    "2017-07-03 19:15:44.279116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0\n",
    "2017-07-03 19:15:44.279129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y\n",
    "2017-07-03 19:15:44.279145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)\n",
    "2017-07-03 19:15:50.506132: precision @ 1 = 0.784\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Option]  Using TensorBoard\n",
    "\n",
    "```\n",
    "tensorboard --logdir $EFS_MOUNT/deeplearning-cfn/examples/tensorflow/train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "* Distributed TensorFlow https://www.tensorflow.org/deploy/distributed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
